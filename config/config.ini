########################
## CONFIGURATION FILE ##
########################

[TEXT_SOURCES]
# Path to source files. This can be a path to TEI or plain text files or a path to a PhiloLogic database.
source_file_path =

# Path to metadata for plain text source files. Needs to be a CSV or TSV file with at least the filename as metadata
source_metadata =

# Path to target files. This can be a path to to TEI or plain text files or a path to a PhiloLogic database.
target_file_path =

# Path to metadata for plain text target files. Needs to be a CSV or TSV file with at least the filename as metadata
target_metadata =

# For backwards compatibility. Will remove in future versions
source_url =
target_url =

[TEXT_PARSING]
##########################################################################
## If TEI parsing was not done by PhiloLogic, you can parse your source ##
## and target files directly from TextPAIR                              ##
##########################################################################

# Defines whether to parse source files
parse_source_files = yes

# Source files type: specify tei for TEI files, and plain_text for plain-text files.
source_file_type = tei

# Defines path to file containing words to keep (useful for dirty OCR)
# Default is keeping all words
source_words_to_keep = all

# Defines whether to parse target files
parse_target_files = yes

# Target files type: specify tei for TEI files, and plain_text for plain-text files.
target_file_type = tei

# Defines path to file containing words to keep (useful for dirty OCR)
# Default is keeping all words
target_words_to_keep = all


[PREPROCESSING]
# Defines what object type to divide each text into
# Useful to break up a single document into smaller text units
source_text_object_type = doc
target_text_object_type = doc

# Defines how many tokens constitute a ngram
ngram = 3

# Defines size of gap autorized in ngram. If not 0, this will generate multiple ngrams within a window size of ngram+gap
# Note that you may need to adjust your minimum number of ngrams for matches to avoid short matches as a result.
# USE WITH CAUTION as this will multiply the RAM usage for your alignment
gap = 0

# The word order must be respected
word_order = yes

# Language: set the language for various normalization tasks
# such as stemming, lemmatizing, word mapping...etc
language =

# Language for target corpus: only set if your source and target corpus are in a different language
# USE ONLY with vsa with transformer vectorization using a multilingual model
target_language =

# Modernize language if modernization is available for your language: currently only French is supported.
modernize = yes

# Transliterate characters to closest ascii representation.
ascii = no

# Stem words using the Porter Stemmer
stemmer = yes

# Lemmatizer: path to lemmatizer file where each line contains the inflected form and
# the corresponding lemma separated by a tab. If set to spacy, make sure to also set spacy_model
lemmatizer =

# Lowercase words
lowercase = yes

# Remove numbers
numbers = yes

# Minimum word length
minimum_word_length = 2

# Stopwords: path to stopword list
stopwords =

# Define a language model to use for lemmatization, and POS tagging
# See https://spacy.io/models for available models. Make sure to download the model first
spacy_model =

# Parts-of-speech to keep: specify which parts of speach to keep. Use Universal POS tag notation. See here for a complete list:
# https://universaldependencies.org/docs/u/pos/
# Separate each pos to keep by a comma
pos_to_keep =

#######################################################################
### VECTOR SPACE ALIGNMENT preprocessing options: VERY EXPERIMENTAL ###
#######################################################################

# If set to n_token, text object is constitued of n_tokens where n is min_text_object_length.
# if set to text_object, text objects are defined by their level in the OHCO hierarchy as defined in source_text_object_type and
# target_text_object_type.
text_object_definition = n_token

# Minimum size of text object length to be counted as a chunk
min_text_object_length = 10

# Defines how many text object should constitute a text chunk used for similarity comparison.
n_chunk = 3

# Vectorization method: either tfidf, w2v, or transformer
vectorization = tfidf

# Minimum frequency of token: expressed as a floating number between 0 and 1
min_freq = 0.05

# Maximum frequency of token: expressed as a floating number between 0 and 1
max_freq = 0.9

# Model used for creating a document embedding: required if using w2v or transformer vectorization
# if using w2v vectorization, use a Spacy model
# if using transformer, use a Hugging Face transformer model (supported by sentence-transformers)
embedding_model =


[LLM_PARAMS]
##############################
## GENERATIVE AI SETTINGS  ##
##############################
# Generative AI mode to re-evaluate similarity of passage pairs
# Uses Llama-cpp under the hood so requires a local model or a Hugging Face model
llm_model =

# Context window size for the LLM model. Must be equal to or smaller than the model's context window
# Note that you may also be limited by your GPU VRAM if using a GPU
llm_context_window = 8192

# llm server port: only change if you have a port conflict
llm_port = 8080

# Concurrency limit for LLM requests: increase if you have the RAM/VRAM to handle more concurrent requests
llm_concurrency_limit = 8


[MATCHING]
########################
## PROCESSING OPTIONS ##
########################

# Matching algorithm used to find similar passage pairs. Current options are
# sa (for sequence alignment), and vsa (for vector space alignment).
# DON'T USE vsa at this time, it may not work at all.
matching_algorithm = sa

# Sort files prior to matching. This may be important when wanting to avoid
# comparing a source file with a target file that occurs later in time
sort_by = year

# Defines in how many batches your source or target corpus will be loaded: useful if your corpus is too big to fit in RAM
# The default of 1 is to process the whole corpus at once.
source_batch = 1
target_batch = 1

# Size of left and right context in bytes
context_size = 300

#########################
## MATCHING PARAMETERS ##
#########################

# Size of ngram window to be initially evaluated in the sequence aligner
matching_window_size = 30

# Minimum number of shared ngrams between docs to start a comparison
minimum_matching_ngrams_in_docs = 4

# Percentage of shared ngrams between 2 docs to consider the target as a duplicate of source
duplicate_threshold = 80

# Minimum number of matching ngrams in ngram window
minimum_matching_ngrams_in_window = 4

# Maximum gap authorized between matching ngrams
max_gap = 15

# Minimum number of matching ngrams to constitute a match
minimum_matching_ngrams  = 4

# Automatically increase max_gap once minimum_matching_ngrams is reached
flex_gap = true

# ONLY FOR VSA: defines similarity threshold for initial matching. Value between 0 and 1, with values closer to one
# meaning higher similarity.
min_similarity = 0.5

# ONLY FOR VSA: minimum number of matching words: this is to make sure your match is not based on just a couple of
# highly weighted words
min_matching_words = 5

# Use LLM to re-evaluate similarity of passage pairs found by the initial matching algorithm
llm_eval = false

# Similarity threshold for the LLM to keep a passage pair as a match
# if no value is provided, will default to min_similarity used for initial matching
llm_similarity_threshold = 0.75

# Output the reasoning of the LLM for each evaluation to a debug file
# Debut file will be in output/debug/llm_evaluations.txt
llm_debug = false

###################################
## PASSAGE MERGING AND EXTENDING ##
###################################

# Merge passages within n number of byte: number defined by passage length and the passage_distance_multiplier option.
merge_passages_on_byte_distance = true

# Combine passage which are within (multiplier * length of previous passage) bytes. Needs merge_passages_on_byte_distance set to true
passage_distance_multiplier = 0.5

# Merge passages within n number of ngrams: the value used is the matching_window_size defaulting to 20
merge_passages_on_ngram_distance = true

#################################
## BANALITY DETECTION SETTINGS ##
#################################
# Whether to detect banalities, or formulaic expressions automatically
banality_auto_detection = true

# Whether to use the LLM to re-evaluate banalities detected by the automatic detection
banality_llm_post_eval = false

# Defines how many of the top ngrams we use to determine if a passage is a banality
# The value is expressed as a percentage.
most_common_ngram_proportion = 0.1

# Expressed in percentage of ngrams flagged as common. Beyond that percentage, passages are
# flagged as banalities.
common_ngram_threshold = 50

# Whether to store or dismiss formulaic expressions. If not stored, these are
# saved in a separate file for further examination
store_banalities = false

# Path to file containing phrases used to flag banalities and non-interesting matches
# Note that all matches are removed and saved in a separate file
# Also of note, this filter will remove any passage which contains an instance of a phrase
phrase_filter =


[PASSAGE_CLASSIFICATION]
###################################
## PASSAGE_CLASSIFICATION SETTINGS ##
###################################
# Whether to classify passages into thematic categories using a zero-shot transformer model
classify_passage = false

# Zero-shot model to use for classification. Should be a Hugging Face model compatible with the pipeline
# See https://huggingface.co/models?pipeline_tag=zero-shot-classification
zero_shot_model =

# Define each class and the criteria to use to classify matches following the below model
Satire & Humor = "Passages primarily using irony, satire, humor, parody, or comical situations to critique or entertain. Focus on the context of enunciation: the text must clearly mock or parody an important theme; stylistic choices are aimed at creating comic effects and exaggeration. It is not enough for the text to have a polemical intent; it must also display stylistic qualities that reveal a comic détournement of the original idea, a clear intention to ironize.",
Religion & Spirituality = "Speech about faith, God, theology, scripture, church, sin, redemption, prayer, miracles, saints, religious practice, religious doubt, mysticism. It is not enough for this theme to be merely present — what matters is that it becomes the object of explicit reflection.",
Philosophy = "Speech about morality, ethics, virtue, reason, metaphysics, logic, existence, knowledge, truth, justice (as a concept), free will, nature of humanity, ethical dilemmas. It is not enough for this theme to be merely present — what matters is that it becomes the object of explicit reflection.",
Politics, Law, & Governance = "Speech about power and its nature, the state, specific laws/decrees (their content), rights, citizenship and citizen participation, social order, revolution, political factions, diplomacy, governance, monarchy, republic. It is not enough for this theme to be merely present — what matters is that it becomes the object of explicit reflection.",
History & Memory = "Passages describing battles, military life, strategy, soldiers, heroism, the impacts of war, civil unrest, duels. They could include references to specific historical events or figures, chronicles, discussion of the past, memory, tradition, national identity. Unlike the Social & Cultural Commentary category, it is important that the focus is placed on major events, prominent figures, and the great themes in the history of nations or peoples.",
Social & Cultural Commentary = "Observations or critiques of society, class structure, customs, manners, social norms, inequality, poverty, public behavior, specific social groups. The themes can also be other ones, relating of everyday activities, work, food, clothing, housing, common rituals, non-political/non-religious customs to practices of love or marriage, family and friendship, funeral practices or the sense of time. Even accounts of journeys, voyages, geographical discoveries, descriptions of foreign lands or peoples fall into this category. Unlike the History & Memory category, the focus is more strictly sociological, concerning the lives of individuals, their concrete practices, and their ideas about everyday life.",
Nature & Science = "Any purely descriptions of the natural world (landscapes, animals, weather), natural philosophy, scientific thought, discovery, medicine, technology. Focus on the strictly descriptive aspect: whether it is the description of nature or of scientific practices, it should appear as detached as possible. The presence of appropriate naturalistic or scientific vocabulary is central to assigning the text to this category. Pay close attention to the context of enunciation — the text should clearly present itself as objective, observational, and uninvolved.",
Art & Literature = "Discussions, analysis, commentary, or critique about literature (authors, works, characters, genres, style, rhetoric) or other art forms (visual arts, music, theatre, aesthetics, artists). Do not focus on the literary quality of the text, but only on its metatextual aspect — on the development of commentary and analysis concerning literature and art."