########################
## CONFIGURATION FILE ##
########################

[FILE_PATHS]
# Path to source files. This can be a path to TEI files or a path to a PhiloLogic database.
source_file_path =

# Path to target files. This can be a path to TEI files or a path to a PhiloLogic database.
target_file_path =

[TEI_PARSING]
##########################################################################
## If TEI parsing was not done by an external process (eg. PhiloLogic), ##
## you can parse your source and target files using the provided xml    ##
## parser in utils/                                                     ##
##########################################################################

# Defines whether to parse source files
parse_source_files = yes

# Defines path to file containing words to keep (useful for dirty OCR)
# Default is keeping all words
source_words_to_keep = all

# Defines whether to parse target files
parse_target_files = yes

# Defines path to file containing words to keep (useful for dirty OCR)
# Default is keeping all words
target_words_to_keep = all


[PREPROCESSING]
# Defines what object level to divide each text into
# Only posssible with a Philologic4 index
# Useful to break up a single document into smaller text units
source_text_object_level = doc
target_text_object_level = doc

# Defines how many tokens constitute a ngram
ngram = 3

# Defines size of gap autorized in ngram. If not 0, this will generate multiple ngrams within a window size of ngram+gap
# Note that you may need to adjust your minimum number of ngrams for matches to avoid short matches as a result.
# USE WITH CAUTION as this will multiply the RAM usage for your alignment
gap = 0

# The word order must be respected
word_order = yes

# Language: set the language for various normalization tasks
# such as stemming, lemmatizing, word mapping...etc
language =

# Language for target corpus: only set if your source and target corpus are in a different language
# USE ONLY with vsa with transformer vectorization using a multilingual model
target_language =

# Modernize language if modernization is available for your language: currently only French is supported.
modernize = yes

# Transliterate characters to closest ascii representation.
ascii = no

# Stem words using the Porter Stemmer
stemmer = yes

# Lemmatizer: path to lemmatizer file where each line contains the inflected form and
# the corresponding lemma separated by a tab
lemmatizer =

# Lowercase words
lowercase = yes

# Remove numbers
numbers = yes

# Minimum word length
minimum_word_length = 2

# Stopwords: path to stopword list
stopwords =

# Parts-of-speech to keep: specify which parts of speach to keep. Use Universal POS tag notation. See here for a complete list:
# https://universaldependencies.org/docs/u/pos/
# Separate each pos to keep by a comma
pos_to_keep =

#######################################################################
### VECTOR SPACE ALIGNMENT preprocessing options: VERY EXPERIMENTAL ###
#######################################################################

# If set to n_token, text object is constitued of n_tokens where n is min_text_object_length.
# if set to text_object, text objects are defined by their level in the OHCO hierarchy as defined in source_text_object_level and
# target_text_object_level.
text_object_definition = n_token

# Split text chunks on text object level: e.g. to avoid grouping together sentences from
# different divs, paragraphs... etc
text_object_level_split = doc

# Minimum size of text object length to be counted as a chunk
min_text_object_length = 10

# Defines how many text object should constitute a text chunk used for similarity comparison.
n_chunk = 3

# Vectorization method: either tfidf, w2v, or transformer
vectorization = tfidf

# Minimum frequency of token: expressed as a floating number between 0 and 1
min_freq = 0.05

# Maximum frequency of token: expressed as a floating number between 0 and 1
max_freq = 0.9

# Model used for creating a document embedding: required if using w2v or transformer vectorization
# if using w2v vectorization, use a Spacy model
# if using transformer, use a Hugging Face transformer model (supported by sentence-transformers)
model_name =


[MATCHING]
########################
## PROCESSING OPTIONS ##
########################

# Matching algorithm used to find similar passage pairs. Current options are
# sa (for sequence alignment), and vsa (for vector space alignment).
# DON'T USE vsa at this time, it may not work at all.
matching_algorithm = sa

# Sort files prior to matching. This may be important when wanting to avoid
# comparing a source file with a target file that occurs later in time
sort_by = year

# Defines in how many batches your source or target corpus will be loaded: useful if your corpus is too big to fit in RAM
# The default of 1 is to process the whole corpus at once.
source_batch = 1
target_batch = 1

# Size of left and right context in bytes
context_size = 300

#########################
## MATCHING PARAMETERS ##
#########################

# Size of ngram window to be initially evaluated in the sequence aligner
matching_window_size = 30

# Minimum number of shared ngrams between docs to start a comparison
minimum_matching_ngrams_in_docs = 4

# Percentage of shared ngrams between 2 docs to consider the target as a duplicate of source
duplicate_threshold = 80

# Minimum number of matching ngrams in ngram window
minimum_matching_ngrams_in_window = 4

# Maximum gap authorized between matching ngrams
max_gap = 15

# Minimum number of matching ngrams to constitute a match
minimum_matching_ngrams  = 4

# Automatically increase max_gap once minimum_matching_ngrams is reached
flex_gap = false

# ONLY FOR VSA: defines similarity threshold. Value between 0 and 1, with values closer to one
# meaning higher similarity.
min_similarity = 0.5

# ONLY FOR VSA: similarity metric to be used. See Gensim docs for metrics available.
similarity_metric = cosine

# ONLY FOR VSA: minimum number of matching words: this is to make sure your match is not based on just a couple of
# highly weighted words
min_matching_words = 5

###################################
## PASSAGE MERGING AND EXTENDING ##
###################################

# Merge passages within n number of byte: number defined by passage length and the passage_distance_multiplier option.
merge_passages_on_byte_distance = true

# Combine passage which are within (multiplier * length of previous passage) bytes. Needs merge_passages_on_byte_distance set to true
passage_distance_multiplier = 0.5

# Merge passages within n number of ngrams: the value used is the matching_window_size defaulting to 20
merge_passages_on_ngram_distance = true

#################################
## BANALITY DETECTION SETTINGS ##
#################################
# Whether to detect banalities, or formulaic expressions automatically
banality_auto_detection = true

# Whether to store or dismiss formulaic expressions. If not stored, these are
# saved in a separate file for further examination
store_banalities = false

# Path to file containing phrases used to flag banalities and non-interesting matches
# Note that all matches are removed and saved in a separate file
phrase_filter =


[WEB_APPLICATION]
##################################
#### WEB APP LOADING OPTIONS ####
##################################

# URL of API server
api_server = http://localhost/text-pair-api/

# In the following section, set the type for fields when stored in PostgreSQL
# Supported types are TEXT and INTEGER. If the field is not set, it will default to text.
source_year = INTEGER
target_year = INTEGER

# Link aligments to PhiloLogic databases
source_philo_db_link =
target_philo_db_link =
