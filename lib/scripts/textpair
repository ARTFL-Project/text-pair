#!/usr/bin/env python3
"""Sequence aligner script"""

import os

from textpair import (
    Ngrams,
    create_web_app,
    get_config,
    run_vsa,
    banality_auto_detect,
    phrase_matcher,
    parse_files,
    merge_alignments,
)


def get_count(path: str) -> int:
    if os.path.exists(path):
        with open(path, encoding="utf8") as input_file:
            count = int(input_file.read().strip())
    else:
        count = 0  # TODO: handle this case by counting matches in results file
    return count


def update_count(count: int, to_remove: int, path: str) -> int:
    path = os.path.join(path, "results/count.txt")
    count -= to_remove
    with open(path, "w", encoding="utf8") as output_file:
        output_file.write(str(count))
    return count


def run_alignment(params):
    """Main function to start sequence alignment"""
    if params.only_align is False:
        if params.text_parsing["parse_source_files"] is True:
            print("\n### Parsing source files ###")
            parse_files(
                params.paths["source"]["input_files"],
                params.text_parsing["source_file_type"],
                params.paths["source"]["input_source_metadata"],
                params.paths["source"]["parse_output"],
                params.text_parsing["source_words_to_keep"],
                params.preprocessing_params["source"]["text_object_type"],
                params.workers,
                params.debug,
            )
        print("\n### Generating source ngrams ###")
        ngrams = Ngrams(debug=params.debug, **params.preprocessing_params["source"])
        ngrams.generate(
            params.paths["source"]["input_files_for_ngrams"],
            params.paths["source"]["ngram_output_path"],
            params.workers,
        )
        if params.paths["target"]:
            if params.text_parsing["parse_target_files"] is True:
                print("\n### Parsing target files ###")
                parse_files(
                    params.paths["target"]["input_files"],
                    params.text_parsing["target_file_type"],
                    params.paths["target"]["input_target_metadata"],
                    params.paths["target"]["parse_output"],
                    params.text_parsing["target_words_to_keep"],
                    params.preprocessing_params["target"]["text_object_type"],
                    params.workers,
                    params.debug,
                )
            print("\n### Generating target ngrams ###")
            ngrams = Ngrams(debug=params.debug, **params.preprocessing_params["target"])
            ngrams.generate(
                params.paths["target"]["input_files_for_ngrams"],
                params.paths["target"]["ngram_output_path"],
                params.workers,
            )
    print("\n### Starting sequence alignment ###")
    if params.paths["target"]["ngram_output_path"] == "":  # if path not defined make target like source
        params.paths["target"]["ngram_output_path"] = params.paths["source"]["ngram_output_path"]
    result_batch_path = os.path.join(params.output_path, "results/result_batches")
    if os.path.exists(result_batch_path):
        os.system(f"rm -rf {result_batch_path}")
    command = f"""compareNgrams \
                --output_path={params.output_path}/results \
                --threads={params.workers} \
                --source_files={params.paths["source"]["ngram_output_path"]}/ngrams \
                --target_files={params.paths["target"]["ngram_output_path"]}/ngrams \
                --source_metadata={params.paths["source"]["metadata_path"]} \
                --target_metadata={params.paths["target"]["metadata_path"]} \
                --sort_by={params.matching_params["sort_by"]} \
                --source_batch={params.matching_params["source_batch"]} \
                --target_batch={params.matching_params["target_batch"]} \
                --matching_window_size={params.matching_params["matching_window_size"]} \
                --max_gap={params.matching_params["max_gap"]} \
                --flex_gap={params.matching_params["flex_gap"]} \
                --minimum_matching_ngrams={params.matching_params["minimum_matching_ngrams"]} \
                --minimum_matching_ngrams_in_window={params.matching_params["minimum_matching_ngrams_in_window"]} \
                --minimum_matching_ngrams_in_docs={params.matching_params["minimum_matching_ngrams_in_docs"]} \
                --context_size={params.matching_params["context_size"]} \
                --duplicate_threshold={params.matching_params["duplicate_threshold"]} \
                --merge_passages_on_byte_distance={params.matching_params["merge_passages_on_byte_distance"]} \
                --merge_passages_on_ngram_distance={params.matching_params["merge_passages_on_ngram_distance"]} \
                --passage_distance_multiplier={params.matching_params["passage_distance_multiplier"]} \
                --debug={str(params.debug).lower()} \
                --ngram_index={params.matching_params["ngram_index"]}"""
    results_file = f"{params.output_path}/results/alignments.jsonl.lz4"
    if os.path.exists(results_file):
        os.system(f"rm -rf {results_file}")
    if params.debug:
        print(f"Running alignment with following arguments:\n{' '.join(command.split())}")
    os.system(command)
    if len(os.listdir(result_batch_path)) == 1:
        filename = os.listdir(result_batch_path)[0]
        os.system(f"mv {result_batch_path}/{filename} {results_file} && rm -rf {result_batch_path}")
    else:
        print("Merging alignments into one file (this may take a while)... ", end="", flush=True)
        merge_command = f"find {result_batch_path} -type f | sort -V | xargs lz4cat --rm | lz4 -q > {results_file}; rm -rf {result_batch_path}"
        os.system(merge_command)
        print("done.")
    count = get_count(os.path.join(params.output_path, "results/count.txt"))

    # Postprocessing steps
    if params.matching_params["phrase_filter"] or params.matching_params["banality_auto_detection"] is True:
        print(f"\n### Postprocessing {count} pairwise alignments ###")
        if params.matching_params["phrase_filter"]:
            filtered_passages = phrase_matcher(results_file, params.matching_params["phrase_filter"], count)
            print(f"{filtered_passages} pairwise alignments have been filtered based on the phrase filter provided.")
            count = update_count(count, filtered_passages, params.output_path)
            print(f"{count} pairwise alignments remaining.")
        if params.matching_params["banality_auto_detection"] is True:
            banalities_found = banality_auto_detect(
                results_file,
                params.paths["source"]["common_ngrams"],
                f'{params.paths["source"]["ngram_output_path"]}/ngrams_in_order',
                params.matching_params["store_banalities"],
                count,
                params.matching_params["most_common_ngram_proportion"],
                params.matching_params["common_ngram_threshold"],
            )
            if params.matching_params["store_banalities"] is False:
                print(
                    f"{banalities_found} pairwise alignment(s) have been identified as formulaic and have been removed from matches."
                )
                count = update_count(count, banalities_found, params.output_path)
                print(f"{count} pairwise alignments remaining.")
            else:
                print(
                    f"{banalities_found} pairwise alignments identified as formulaic and will be flagged as banalities in the database."
                )

    # Passage merger
    print("Grouping passages by source...", end="", flush=True)
    groups_file = merge_alignments(results_file, count)

    if params.web_app_config["skip_web_app"] is False:
        create_web_app(
            results_file,
            params.paths["source"]["metadata_path"],
            params.paths["target"]["metadata_path"],
            count,
            params.dbname,
            params.web_app_config["web_application_directory"],
            params.web_app_config["api_server"],
            params.web_app_config["source_url"],
            params.web_app_config["target_url"],
            params.matching_params["matching_algorithm"],
            groups_file=groups_file,
            store_banalities=params.matching_params["store_banalities"],
        )


def run_vsa_similarity(params) -> None:
    """Run vsa similarity"""
    print("\n### Starting vector space alignment ###")
    if params.paths["target"]["ngram_output_path"] == "":  # if path not defined make target like source
        params.paths["target"]["ngram_output_path"] = params.paths["source"]["ngram_output_path"]
    run_vsa(
        params.paths["source"]["input_files_for_ngrams"],
        params.paths["target"]["input_files_for_ngrams"],
        params.workers,
        {**params.preprocessing_params, **params.matching_params},
    )
    if params.web_app_config["skip_web_app"] is False:
        output_file = os.path.join(params.output_path, "results/alignments.jsonl.lz4")
        count = get_count(os.path.join(params.output_path, "results/counts.txt"))
        create_web_app(
            output_file,
            params.paths["source"]["metadata_path"],
            params.paths["target"]["metadata_path"],
            count,
            params.dbname,
            params.web_app_config["web_application_directory"],
            params.web_app_config["api_server"],
            params.web_app_config["source_url"],
            params.web_app_config["target_url"],
            params.matching_params["matching_algorithm"],
        )


if __name__ == "__main__":
    params = get_config()
    if params.update_db is True:
        count = get_count(os.path.join(params.output_path, "results/count.txt"))
        if params.matching_params["matching_algorithm"] == "sa":  # we merge alignments prior to loading
            print("Grouping passages by source...", end="", flush=True)
            groups_file = merge_alignments(params.file, count)
        create_web_app(
            params.file,
            params.paths["source"]["metadata_path"],
            params.paths["target"]["metadata_path"],
            count,
            params.dbname,
            params.web_app_config["web_application_directory"],
            params.web_app_config["api_server"],
            params.web_app_config["source_url"],
            params.web_app_config["target_url"],
            params.matching_params["matching_algorithm"],
            load_only_db=True,
            store_banalities=params.matching_params["store_banalities"],
        )
    elif params.only_web_app is True:
        count = get_count(os.path.join(params.output_path, "results/count.txt"))
        create_web_app(
            params.file,
            params.paths["source"]["metadata_path"],
            params.paths["target"]["metadata_path"],
            count,
            params.dbname,
            params.web_app_config["web_application_directory"],
            params.web_app_config["api_server"],
            params.web_app_config["source_url"],
            params.web_app_config["target_url"],
            params.matching_params["matching_algorithm"],
            store_banalities=params.matching_params["store_banalities"],
        )
    elif params.matching_params["matching_algorithm"] == "sa":
        run_alignment(params)
    elif params.matching_params["matching_algorithm"] == "vsa":
        run_vsa_similarity(params)
